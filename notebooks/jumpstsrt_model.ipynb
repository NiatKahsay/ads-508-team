{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b9d834-59e4-4597-9421-17b392f27dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded from S3. Shape: (70000, 19)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Define your bucket and key (same as your Autopilot data)\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "key = \"feature-store/cardio/cardio-feature-group-22-21-14-34/autopilot_input.csv\"\n",
    "\n",
    "# Create boto3 client and download\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df_with_split = pd.read_csv(obj['Body'])\n",
    "\n",
    "print(\"✅ Data loaded from S3. Shape:\", df_with_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c33861-24b9-498a-918b-1c6b555a005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean file uploaded to: s3://cardiovale-solutions-datascience-pipeline/jumpstart/cardio_clean_noheader.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Load from S3\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "key = \"jumpstart/cardio_clean_noheader.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df = pd.read_csv(obj[\"Body\"], header=None)\n",
    "\n",
    "# Infer & fix types\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except ValueError:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Save cleaned version (no header for JumpStart)\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False, header=False)\n",
    "\n",
    "clean_key = \"jumpstart/cardio_clean_noheader_fixed.csv\"\n",
    "s3.put_object(Bucket=bucket, Key=clean_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"Cleaned file re-uploaded to: s3://{bucket}/{clean_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492bbc13-d899-4bd1-ba5b-a4fc7ddd83e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'xgboost-classification-model' with wildcard version identifier '*'. You can pin to version '2.1.7' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'xgboost-classification-model' with wildcard version identifier '*'. You can pin to version '2.1.7' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "INFO:sagemaker:Creating training-job with name: xgb-classification-model-2025-03-30-23-13-25-609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-30 23:13:28 Starting - Starting the training job...\n",
      "2025-03-30 23:13:46 Starting - Preparing the instances for training...\n",
      "2025-03-30 23:14:04 Downloading - Downloading input data...\n",
      "2025-03-30 23:14:50 Downloading - Downloading the training image......\n",
      "2025-03-30 23:15:51 Training - Training image download completed. Training in progress..\u001b[34m[2025-03-30 23:15:55.822 algo-1:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-03-30 23:15:55.844 algo-1:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Module transfer_learning does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:15:56:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/asn1crypto/asn1crypto-1.5.1-py2.py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/attrs/attrs-23.1.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/boto3/boto3-1.26.158-py3-none-any.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/botocore/botocore-1.29.159-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/certifi/certifi-2023.5.7-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cffi/cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/charset-normalizer/charset_normalizer-2.1.1-py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cloudpickle/cloudpickle-2.2.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/contextlib2/contextlib2-21.6.0-py2.py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cryptography/cryptography-40.0.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dill/dill-0.3.6-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/filelock/filelock-3.12.2-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/google-pasta/google_pasta-0.2.0-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/idna/idna-3.4-py3-none-any.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/importlib-metadata/importlib_metadata-4.13.0-py3-none-any.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/importlib-resources/importlib_resources-5.12.0-py3-none-any.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jmespath/jmespath-1.0.1-py3-none-any.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jsonschema/jsonschema-4.17.3-py3-none-any.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multiprocess/multiprocess-0.70.14-py38-none-any.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/numpy/numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/oscrypto/oscrypto-1.3.0-py2.py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/packaging/packaging-23.1-py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pandas/pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathos/pathos-0.3.0-py3-none-any.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pkgutil-resolve-name/pkgutil_resolve_name-1.3.10-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/platformdirs/platformdirs-3.8.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pox/pox-0.3.2-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ppft/ppft-1.7.6.6-py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/protobuf3-to-dict/protobuf3-to-dict-0.1.5.tar.gz\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/protobuf/protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycparser/pycparser-2.21-py2.py3-none-any.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.12.0.zip\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyjwt/PyJWT-2.7.0-py3-none-any.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyopenssl/pyOpenSSL-23.2.0-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyrsistent/pyrsistent-0.19.3-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytz/pytz-2023.3-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyyaml/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/requests/requests-2.31.0-py3-none-any.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/s3transfer/s3transfer-0.6.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker/sagemaker-2.164.0.tar.gz\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/schema/schema-0.7.5-py2.py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/six/six-1.16.0-py2.py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/smdebug-rulesconfig/smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/snowflake-connector-python/snowflake_connector_python-3.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.6.3-py3-none-any.whl (from -r requirements.txt (line 47))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tzdata/tzdata-2023.3-py2.py3-none-any.whl (from -r requirements.txt (line 48))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/urllib3/urllib3-1.26.16-py2.py3-none-any.whl (from -r requirements.txt (line 49))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zipp/zipp-3.15.0-py3-none-any.whl (from -r requirements.txt (line 50))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.0.1-py2.py3-none-any.whl (from -r requirements.txt (line 51))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_snowflake_script_utilities/sagemaker_jumpstart_snowflake_script_utilities-1.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 52))\u001b[0m\n",
      "\u001b[34mpycparser is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34msix is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transfer_learning, protobuf3-to-dict, pycryptodomex, sagemaker\n",
      "  Building wheel for transfer_learning (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for transfer_learning (setup.py): finished with status 'done'\n",
      "  Created wheel for transfer_learning: filename=transfer_learning-1.0.0-py2.py3-none-any.whl size=77430749 sha256=52bbc27cc2017b91a24fa57ef7a569f887a2c88b6ba45d8d0eba7ffb286b6da6\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-uwdtu9l2/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4014 sha256=5bfb155611d9e0b0c29fa22e9c83689cff08f86c7c915c828f95cb1f36c5a2f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/9d/29/c28e5d16d0105aae0e7cb77f5f7294d1f67e152f7255e24b16\n",
      "  Building wheel for pycryptodomex (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pycryptodomex (setup.py): finished with status 'done'\n",
      "  Created wheel for pycryptodomex: filename=pycryptodomex-3.12.0-cp35-abi3-linux_x86_64.whl size=1960853 sha256=89ca642a53038c5421c6608d96a48e0e311c091c273e6f95253f0ba9d60c8c4a\n",
      "  Stored in directory: /root/.cache/pip/wheels/ef/a3/db/6b613d9261adea3495b8b319a510d6cca03ace44a32185546e\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.164.0-py2.py3-none-any.whl size=1081792 sha256=b3a40c5810c5bbb63274e81ea27af592eec114d73d788ee23462293b3234f862\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/6f/ef/08096f4d03a1d9ae0ebbd2e1744f93c33313afc1f75ce09152\u001b[0m\n",
      "\u001b[34mSuccessfully built transfer_learning protobuf3-to-dict pycryptodomex sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: transfer_learning, pytz, asn1crypto, zipp, urllib3, tzdata, typing-extensions, tblib, smdebug-rulesconfig, sagemaker-jumpstart-snowflake-script-utilities, sagemaker-jumpstart-script-utilities, PyYAML, pyrsistent, PyJWT, pycryptodomex, protobuf, ppft, pox, platformdirs, pkgutil-resolve-name, packaging, oscrypto, numpy, jmespath, idna, filelock, dill, contextlib2, cloudpickle, charset-normalizer, certifi, attrs, schema, requests, python-dateutil, protobuf3-to-dict, multiprocess, importlib-resources, importlib-metadata, google-pasta, cffi, pathos, pandas, jsonschema, cryptography, botocore, s3transfer, pyOpenSSL, snowflake-connector-python, boto3, sagemaker\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.2\n",
      "    Uninstalling pytz-2024.2:\n",
      "      Successfully uninstalled pytz-2024.2\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.20.2\n",
      "    Uninstalling zipp-3.20.2:\n",
      "      Successfully uninstalled zipp-3.20.2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.5\n",
      "    Uninstalling urllib3-1.26.5:\n",
      "      Successfully uninstalled urllib3-1.26.5\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: tblib\n",
      "    Found existing installation: tblib 3.0.0\n",
      "    Uninstalling tblib-3.0.0:\n",
      "      Successfully uninstalled tblib-3.0.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.12\n",
      "    Uninstalling charset-normalizer-2.0.12:\n",
      "      Successfully uninstalled charset-normalizer-2.0.12\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.5.0\n",
      "    Uninstalling importlib_metadata-8.5.0:\n",
      "      Successfully uninstalled importlib_metadata-8.5.0\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.17.1\n",
      "    Uninstalling cffi-1.17.1:\n",
      "      Successfully uninstalled cffi-1.17.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 39.0.1\n",
      "    Uninstalling cryptography-39.0.1:\n",
      "      Successfully uninstalled cryptography-39.0.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.52\n",
      "    Uninstalling botocore-1.20.52:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled botocore-1.20.52\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.7\n",
      "    Uninstalling s3transfer-0.3.7:\n",
      "      Successfully uninstalled s3transfer-0.3.7\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: pyOpenSSL\n",
      "    Found existing installation: pyOpenSSL 24.2.1\n",
      "    Uninstalling pyOpenSSL-24.2.1:\n",
      "      Successfully uninstalled pyOpenSSL-24.2.1\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.52\n",
      "    Uninstalling boto3-1.17.52:\n",
      "      Successfully uninstalled boto3-1.17.52\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-containers 2.8.6.post2 requires typing, which is not installed.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires boto3==1.17.52, but you have boto3 1.26.158 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires botocore==1.20.52, but you have botocore 1.29.159 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires certifi==2023.7.22, but you have certifi 2023.5.7 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires cryptography==39.0.1, but you have cryptography 40.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires numpy==1.24.1, but you have numpy 1.24.3 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires pandas==1.2.4, but you have pandas 2.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires protobuf==3.20.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires python-dateutil==2.8.1, but you have python-dateutil 2.8.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires PyYAML==5.4.1, but you have pyyaml 6.0 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires requests==2.29.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires urllib3==1.26.5, but you have urllib3 1.26.16 which is incompatible.\u001b[0m\n",
      "\u001b[34msmdebug 1.0.29 requires protobuf<=3.20.2,>=3.20.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyJWT-2.7.0 PyYAML-6.0 asn1crypto-1.5.1 attrs-23.1.0 boto3-1.26.158 botocore-1.29.159 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-2.1.1 cloudpickle-2.2.1 contextlib2-21.6.0 cryptography-40.0.2 dill-0.3.6 filelock-3.12.2 google-pasta-0.2.0 idna-3.4 importlib-metadata-4.13.0 importlib-resources-5.12.0 jmespath-1.0.1 jsonschema-4.17.3 multiprocess-0.70.14 numpy-1.24.3 oscrypto-1.3.0 packaging-23.1 pandas-2.0.2 pathos-0.3.0 pkgutil-resolve-name-1.3.10 platformdirs-3.8.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 pyOpenSSL-23.2.0 pycryptodomex-3.12.0 pyrsistent-0.19.3 python-dateutil-2.8.2 pytz-2023.3 requests-2.31.0 s3transfer-0.6.1 sagemaker-2.164.0 sagemaker-jumpstart-script-utilities-1.0.1 sagemaker-jumpstart-snowflake-script-utilities-1.1.0 schema-0.7.5 smdebug-rulesconfig-1.0.1 snowflake-connector-python-3.0.2 tblib-1.7.0 transfer_learning-1.0.0 typing-extensions-4.6.3 tzdata-2023.3 urllib3-1.26.16 zipp-3.15.0\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:16:30:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:16:30:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"colsample_bytree\": \"1\",\n",
      "        \"early_stopping_rounds\": \"30\",\n",
      "        \"gamma\": \"0\",\n",
      "        \"learning_rate\": \"0.3\",\n",
      "        \"max_depth\": \"6\",\n",
      "        \"min_child_weight\": \"1\",\n",
      "        \"num_boost_round\": \"5000\",\n",
      "        \"reg_alpha\": \"0\",\n",
      "        \"reg_lambda\": \"1\",\n",
      "        \"subsample\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"xgb-classification-model-2025-03-30-23-13-25-609\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"colsample_bytree\":\"1\",\"early_stopping_rounds\":\"30\",\"gamma\":\"0\",\"learning_rate\":\"0.3\",\"max_depth\":\"6\",\"min_child_weight\":\"1\",\"num_boost_round\":\"5000\",\"reg_alpha\":\"0\",\"reg_lambda\":\"1\",\"subsample\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":\"1\",\"early_stopping_rounds\":\"30\",\"gamma\":\"0\",\"learning_rate\":\"0.3\",\"max_depth\":\"6\",\"min_child_weight\":\"1\",\"num_boost_round\":\"5000\",\"reg_alpha\":\"0\",\"reg_lambda\":\"1\",\"subsample\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"xgb-classification-model-2025-03-30-23-13-25-609\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"1\",\"--early_stopping_rounds\",\"30\",\"--gamma\",\"0\",\"--learning_rate\",\"0.3\",\"--max_depth\",\"6\",\"--min_child_weight\",\"1\",\"--num_boost_round\",\"5000\",\"--reg_alpha\",\"0\",\"--reg_lambda\",\"1\",\"--subsample\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_COLSAMPLE_BYTREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=30\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=6\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=5000\u001b[0m\n",
      "\u001b[34mSM_HP_REG_ALPHA=0\u001b[0m\n",
      "\u001b[34mSM_HP_REG_LAMBDA=1\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages:/miniconda3/lib/python3.8/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/snowflake/connector/options.py:107: UserWarning: You have an incompatible version of 'pyarrow' installed (14.0.1), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\u001b[0m\n",
      "\u001b[34mINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 254, in <module>\n",
      "    run_with_args(args)\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 186, in run_with_args\n",
      "    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 743, in __init__\n",
      "    handle, feature_names, feature_types = dispatch_data_backend(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 970, in dispatch_data_backend\n",
      "    return _from_pandas_df(data, enable_categorical, missing, threads,\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 417, in _from_pandas_df\n",
      "    data, feature_names, feature_types = _transform_pandas_df(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 391, in _transform_pandas_df\n",
      "    _invalid_dataframe_dtype(data)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 283, in _invalid_dataframe_dtype\n",
      "    raise ValueError(msg)\u001b[0m\n",
      "\u001b[34mValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:feature_12: object, feature_16: object\u001b[0m\n",
      "\u001b[34m[2025-03-30:23:16:32:ERROR] ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\"\u001b[0m\n",
      "\n",
      "2025-03-30 23:16:50 Uploading - Uploading generated training model\n",
      "2025-03-30 23:16:50 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job xgb-classification-model-2025-03-30-23-13-25-609: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m estimator \u001b[38;5;241m=\u001b[39m JumpStartEstimator(\n\u001b[1;32m     21\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     22\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     23\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.m5.large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 5. Launch training job\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/estimator.py:696\u001b[0m, in \u001b[0;36mJumpStartEstimator.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start training job by calling base ``Estimator`` class ``fit`` method.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03mAny field set to ``None`` does not get passed to the parent class method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m        (Default: None).\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m estimator_fit_kwargs \u001b[38;5;241m=\u001b[39m get_fit_kwargs(\n\u001b[1;32m    681\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    682\u001b[0m     model_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m     config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name,\n\u001b[1;32m    694\u001b[0m )\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJumpStartEstimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mestimator_fit_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_kwargs_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:1350\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:2720\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2720\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5853\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5834\u001b[0m \n\u001b[1;32m   5835\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5851\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5853\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8455\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8452\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   8454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 8455\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   8457\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job xgb-classification-model-2025-03-30-23-13-25-609: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\", exit code: 1"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# 1. Setup essentials\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "input_key = \"jumpstart/cardio_clean_noheader.csv\"\n",
    "input_s3_uri = f\"s3://{bucket}/{input_key}\"\n",
    "\n",
    "# 2. Use JumpStart model ID for XGBoost (or replace with other tabular model)\n",
    "model_id = \"xgboost-classification-model\"  # pretrained JumpStart XGBoost\n",
    "\n",
    "# 3. Define training input location\n",
    "input_data = {\n",
    "    \"training\": input_s3_uri\n",
    "}\n",
    "\n",
    "# 4. Configure the JumpStart Estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# 5. Launch training job\n",
    "estimator.fit(inputs=input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3113b980-ad8d-48cc-8a29-a9eb2eef93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded from S3. Shape: (70000, 19)\n",
      "\n",
      "✅ Final cleaned types:\n",
      " cardio                        int64\n",
      "id                            int64\n",
      "age                           int64\n",
      "gender                        int64\n",
      "height                        int64\n",
      "weight                      float64\n",
      "ap_hi                         int64\n",
      "ap_lo                         int64\n",
      "cholesterol                   int64\n",
      "gluc                          int64\n",
      "smoke                         int64\n",
      "alco                          int64\n",
      "active                        int64\n",
      "State                      category\n",
      "offered_for_free            float64\n",
      "medical_restrictions        float64\n",
      "number_of_weeks_offered     float64\n",
      "event_time                 category\n",
      "dtype: object\n",
      "\n",
      "✅ Cleaned dataset re-uploaded to: s3://cardiovale-solutions-datascience-pipeline/jumpstart/cardio_clean_noheader_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and clean dataset for JumpStart\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Load Autopilot dataset from S3\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "key = \"feature-store/cardio/cardio-feature-group-22-21-14-34/autopilot_input.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df_with_split = pd.read_csv(obj[\"Body\"])\n",
    "print(\"✅ Data loaded from S3. Shape:\", df_with_split.shape)\n",
    "\n",
    "# Drop split_type column and reorder cardio to be first\n",
    "df = df_with_split.drop(columns=[\"split_type\"], errors=\"ignore\").copy()\n",
    "cols = list(df.columns)\n",
    "cols.insert(0, cols.pop(cols.index(\"cardio\")))\n",
    "df = df[cols]\n",
    "\n",
    "# Fix data types: convert object → numeric or category\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except ValueError:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "print(\"\\n✅ Final cleaned types:\\n\", df.dtypes)\n",
    "\n",
    "# Save to CSV (no header, as required by JumpStart)\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False, header=False)\n",
    "\n",
    "# Upload clean file to S3\n",
    "clean_key = \"jumpstart/cardio_clean_noheader_fixed.csv\"\n",
    "s3.put_object(Bucket=bucket, Key=clean_key, Body=csv_buffer.getvalue())\n",
    "print(f\"\\n✅ Cleaned dataset re-uploaded to: s3://{bucket}/{clean_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a16a12-b88e-4001-a80f-ab237a2297d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'xgboost-classification-model' with wildcard version identifier '*'. You can pin to version '2.1.7' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "WARNING:sagemaker.jumpstart:Using model 'xgboost-classification-model' with wildcard version identifier '*'. You can pin to version '2.1.7' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Launching JumpStart model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgb-classification-model-2025-03-31-01-18-27-080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-31 01:18:29 Starting - Starting the training job...\n",
      "2025-03-31 01:18:46 Starting - Preparing the instances for training...\n",
      "2025-03-31 01:19:07 Downloading - Downloading input data...\n",
      "2025-03-31 01:19:57 Downloading - Downloading the training image......\n",
      "2025-03-31 01:20:58 Training - Training image download completed. Training in progress..\u001b[34m[2025-03-31 01:21:01.089 algo-1:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-03-31 01:21:01.112 algo-1:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:01:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:01:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:01:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:02:INFO] Module transfer_learning does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:02:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:02:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:02:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/asn1crypto/asn1crypto-1.5.1-py2.py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/attrs/attrs-23.1.0-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/boto3/boto3-1.26.158-py3-none-any.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/botocore/botocore-1.29.159-py3-none-any.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/certifi/certifi-2023.5.7-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cffi/cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/charset-normalizer/charset_normalizer-2.1.1-py3-none-any.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cloudpickle/cloudpickle-2.2.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/contextlib2/contextlib2-21.6.0-py2.py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cryptography/cryptography-40.0.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dill/dill-0.3.6-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/filelock/filelock-3.12.2-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/google-pasta/google_pasta-0.2.0-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/idna/idna-3.4-py3-none-any.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/importlib-metadata/importlib_metadata-4.13.0-py3-none-any.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/importlib-resources/importlib_resources-5.12.0-py3-none-any.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jmespath/jmespath-1.0.1-py3-none-any.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/jsonschema/jsonschema-4.17.3-py3-none-any.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multiprocess/multiprocess-0.70.14-py38-none-any.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/numpy/numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/oscrypto/oscrypto-1.3.0-py2.py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/packaging/packaging-23.1-py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pandas/pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathos/pathos-0.3.0-py3-none-any.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pkgutil-resolve-name/pkgutil_resolve_name-1.3.10-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/platformdirs/platformdirs-3.8.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pox/pox-0.3.2-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ppft/ppft-1.7.6.6-py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/protobuf3-to-dict/protobuf3-to-dict-0.1.5.tar.gz\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/protobuf/protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycparser/pycparser-2.21-py2.py3-none-any.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.12.0.zip\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyjwt/PyJWT-2.7.0-py3-none-any.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyopenssl/pyOpenSSL-23.2.0-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyrsistent/pyrsistent-0.19.3-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/python-dateutil/python_dateutil-2.8.2-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytz/pytz-2023.3-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyyaml/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/requests/requests-2.31.0-py3-none-any.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/s3transfer/s3transfer-0.6.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker/sagemaker-2.164.0.tar.gz\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/schema/schema-0.7.5-py2.py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/six/six-1.16.0-py2.py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/smdebug-rulesconfig/smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/snowflake-connector-python/snowflake_connector_python-3.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.6.3-py3-none-any.whl (from -r requirements.txt (line 47))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tzdata/tzdata-2023.3-py2.py3-none-any.whl (from -r requirements.txt (line 48))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/urllib3/urllib3-1.26.16-py2.py3-none-any.whl (from -r requirements.txt (line 49))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zipp/zipp-3.15.0-py3-none-any.whl (from -r requirements.txt (line 50))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.0.1-py2.py3-none-any.whl (from -r requirements.txt (line 51))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_snowflake_script_utilities/sagemaker_jumpstart_snowflake_script_utilities-1.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 52))\u001b[0m\n",
      "\u001b[34mpycparser is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34msix is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transfer_learning, protobuf3-to-dict, pycryptodomex, sagemaker\n",
      "  Building wheel for transfer_learning (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for transfer_learning (setup.py): finished with status 'done'\n",
      "  Created wheel for transfer_learning: filename=transfer_learning-1.0.0-py2.py3-none-any.whl size=77430749 sha256=5a52a7791928e4b5a6153d3520f03dfe968ca4cd7a88942986f2b028ca1f7a2e\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2r56asco/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4014 sha256=9fd77696b8cb2f548b99d62ab3845401aada05b45ed013cb0a50ad72f1135ab2\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/9d/29/c28e5d16d0105aae0e7cb77f5f7294d1f67e152f7255e24b16\n",
      "  Building wheel for pycryptodomex (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pycryptodomex (setup.py): finished with status 'done'\n",
      "  Created wheel for pycryptodomex: filename=pycryptodomex-3.12.0-cp35-abi3-linux_x86_64.whl size=1960820 sha256=9966d306929ad595c2ae1d4cf3332a633e8a9aeaff9264cbf5f14a37b0efa9a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/ef/a3/db/6b613d9261adea3495b8b319a510d6cca03ace44a32185546e\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.164.0-py2.py3-none-any.whl size=1081792 sha256=4609db50aae5e8e822b70a7541d13fd1db4a20290db35eea15fb116a5fcd0cf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/6f/ef/08096f4d03a1d9ae0ebbd2e1744f93c33313afc1f75ce09152\u001b[0m\n",
      "\u001b[34mSuccessfully built transfer_learning protobuf3-to-dict pycryptodomex sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: transfer_learning, pytz, asn1crypto, zipp, urllib3, tzdata, typing-extensions, tblib, smdebug-rulesconfig, sagemaker-jumpstart-snowflake-script-utilities, sagemaker-jumpstart-script-utilities, PyYAML, pyrsistent, PyJWT, pycryptodomex, protobuf, ppft, pox, platformdirs, pkgutil-resolve-name, packaging, oscrypto, numpy, jmespath, idna, filelock, dill, contextlib2, cloudpickle, charset-normalizer, certifi, attrs, schema, requests, python-dateutil, protobuf3-to-dict, multiprocess, importlib-resources, importlib-metadata, google-pasta, cffi, pathos, pandas, jsonschema, cryptography, botocore, s3transfer, pyOpenSSL, snowflake-connector-python, boto3, sagemaker\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.2\n",
      "    Uninstalling pytz-2024.2:\n",
      "      Successfully uninstalled pytz-2024.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.20.2\n",
      "    Uninstalling zipp-3.20.2:\n",
      "      Successfully uninstalled zipp-3.20.2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.5\n",
      "    Uninstalling urllib3-1.26.5:\n",
      "      Successfully uninstalled urllib3-1.26.5\n",
      "  Attempting uninstall: tblib\n",
      "    Found existing installation: tblib 3.0.0\n",
      "    Uninstalling tblib-3.0.0:\n",
      "      Successfully uninstalled tblib-3.0.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.12\n",
      "    Uninstalling charset-normalizer-2.0.12:\n",
      "      Successfully uninstalled charset-normalizer-2.0.12\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.5.0\n",
      "    Uninstalling importlib_metadata-8.5.0:\n",
      "      Successfully uninstalled importlib_metadata-8.5.0\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.17.1\n",
      "    Uninstalling cffi-1.17.1:\n",
      "      Successfully uninstalled cffi-1.17.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\u001b[0m\n",
      "\u001b[34m    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 39.0.1\n",
      "    Uninstalling cryptography-39.0.1:\n",
      "      Successfully uninstalled cryptography-39.0.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.52\n",
      "    Uninstalling botocore-1.20.52:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled botocore-1.20.52\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.7\n",
      "    Uninstalling s3transfer-0.3.7:\n",
      "      Successfully uninstalled s3transfer-0.3.7\n",
      "  Attempting uninstall: pyOpenSSL\n",
      "    Found existing installation: pyOpenSSL 24.2.1\n",
      "    Uninstalling pyOpenSSL-24.2.1:\n",
      "      Successfully uninstalled pyOpenSSL-24.2.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.52\n",
      "    Uninstalling boto3-1.17.52:\n",
      "      Successfully uninstalled boto3-1.17.52\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-containers 2.8.6.post2 requires typing, which is not installed.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires boto3==1.17.52, but you have boto3 1.26.158 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires botocore==1.20.52, but you have botocore 1.29.159 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires certifi==2023.7.22, but you have certifi 2023.5.7 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires cryptography==39.0.1, but you have cryptography 40.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires numpy==1.24.1, but you have numpy 1.24.3 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires pandas==1.2.4, but you have pandas 2.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires protobuf==3.20.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires python-dateutil==2.8.1, but you have python-dateutil 2.8.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires PyYAML==5.4.1, but you have pyyaml 6.0 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires requests==2.29.0, but you have requests 2.31.0 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires urllib3==1.26.5, but you have urllib3 1.26.16 which is incompatible.\u001b[0m\n",
      "\u001b[34msmdebug 1.0.29 requires protobuf<=3.20.2,>=3.20.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyJWT-2.7.0 PyYAML-6.0 asn1crypto-1.5.1 attrs-23.1.0 boto3-1.26.158 botocore-1.29.159 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-2.1.1 cloudpickle-2.2.1 contextlib2-21.6.0 cryptography-40.0.2 dill-0.3.6 filelock-3.12.2 google-pasta-0.2.0 idna-3.4 importlib-metadata-4.13.0 importlib-resources-5.12.0 jmespath-1.0.1 jsonschema-4.17.3 multiprocess-0.70.14 numpy-1.24.3 oscrypto-1.3.0 packaging-23.1 pandas-2.0.2 pathos-0.3.0 pkgutil-resolve-name-1.3.10 platformdirs-3.8.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 pyOpenSSL-23.2.0 pycryptodomex-3.12.0 pyrsistent-0.19.3 python-dateutil-2.8.2 pytz-2023.3 requests-2.31.0 s3transfer-0.6.1 sagemaker-2.164.0 sagemaker-jumpstart-script-utilities-1.0.1 sagemaker-jumpstart-snowflake-script-utilities-1.1.0 schema-0.7.5 smdebug-rulesconfig-1.0.1 snowflake-connector-python-3.0.2 tblib-1.7.0 transfer_learning-1.0.0 typing-extensions-4.6.3 tzdata-2023.3 urllib3-1.26.16 zipp-3.15.0\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:37:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:37:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"colsample_bytree\": \"1\",\n",
      "        \"early_stopping_rounds\": \"30\",\n",
      "        \"gamma\": \"0\",\n",
      "        \"learning_rate\": \"0.3\",\n",
      "        \"max_depth\": \"6\",\n",
      "        \"min_child_weight\": \"1\",\n",
      "        \"num_boost_round\": \"5000\",\n",
      "        \"reg_alpha\": \"0\",\n",
      "        \"reg_lambda\": \"1\",\n",
      "        \"subsample\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"xgb-classification-model-2025-03-31-01-18-27-080\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"colsample_bytree\":\"1\",\"early_stopping_rounds\":\"30\",\"gamma\":\"0\",\"learning_rate\":\"0.3\",\"max_depth\":\"6\",\"min_child_weight\":\"1\",\"num_boost_round\":\"5000\",\"reg_alpha\":\"0\",\"reg_lambda\":\"1\",\"subsample\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":\"1\",\"early_stopping_rounds\":\"30\",\"gamma\":\"0\",\"learning_rate\":\"0.3\",\"max_depth\":\"6\",\"min_child_weight\":\"1\",\"num_boost_round\":\"5000\",\"reg_alpha\":\"0\",\"reg_lambda\":\"1\",\"subsample\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"xgb-classification-model-2025-03-31-01-18-27-080\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"1\",\"--early_stopping_rounds\",\"30\",\"--gamma\",\"0\",\"--learning_rate\",\"0.3\",\"--max_depth\",\"6\",\"--min_child_weight\",\"1\",\"--num_boost_round\",\"5000\",\"--reg_alpha\",\"0\",\"--reg_lambda\",\"1\",\"--subsample\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_COLSAMPLE_BYTREE=1\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=30\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=6\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=5000\u001b[0m\n",
      "\u001b[34mSM_HP_REG_ALPHA=0\u001b[0m\n",
      "\u001b[34mSM_HP_REG_LAMBDA=1\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages:/miniconda3/lib/python3.8/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/snowflake/connector/options.py:107: UserWarning: You have an incompatible version of 'pyarrow' installed (14.0.1), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\u001b[0m\n",
      "\u001b[34mINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 254, in <module>\n",
      "    run_with_args(args)\n",
      "  File \"/opt/ml/code/transfer_learning.py\", line 186, in run_with_args\n",
      "    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 743, in __init__\n",
      "    handle, feature_names, feature_types = dispatch_data_backend(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 970, in dispatch_data_backend\n",
      "    return _from_pandas_df(data, enable_categorical, missing, threads,\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 417, in _from_pandas_df\n",
      "    data, feature_names, feature_types = _transform_pandas_df(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 391, in _transform_pandas_df\n",
      "    _invalid_dataframe_dtype(data)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/data.py\", line 283, in _invalid_dataframe_dtype\n",
      "    raise ValueError(msg)\u001b[0m\n",
      "\u001b[34mValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:feature_12: object, feature_16: object\u001b[0m\n",
      "\u001b[34m[2025-03-31:01:21:39:ERROR] ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\"\u001b[0m\n",
      "\n",
      "2025-03-31 01:21:57 Uploading - Uploading generated training model\n",
      "2025-03-31 01:21:57 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job xgb-classification-model-2025-03-31-01-18-27-080: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m estimator \u001b[38;5;241m=\u001b[39m JumpStartEstimator(\n\u001b[1;32m     19\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m     20\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     21\u001b[0m     instance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml.m5.large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39msagemaker_session\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Launching JumpStart model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/estimator.py:696\u001b[0m, in \u001b[0;36mJumpStartEstimator.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start training job by calling base ``Estimator`` class ``fit`` method.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03mAny field set to ``None`` does not get passed to the parent class method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m        (Default: None).\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m estimator_fit_kwargs \u001b[38;5;241m=\u001b[39m get_fit_kwargs(\n\u001b[1;32m    681\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    682\u001b[0m     model_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m     config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name,\n\u001b[1;32m    694\u001b[0m )\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJumpStartEstimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mestimator_fit_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_kwargs_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:1350\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:2720\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2720\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5853\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5834\u001b[0m \n\u001b[1;32m   5835\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5851\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5853\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8455\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8452\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   8454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 8455\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   8457\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job xgb-classification-model-2025-03-31-01-18-27-080: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m transfer_learning --colsample_bytree 1 --early_stopping_rounds 30 --gamma 0 --learning_rate 0.3 --max_depth 6 --min_child_weight 1 --num_boost_round 5000 --reg_alpha 0 --reg_lambda 1 --subsample 1\", exit code: 1"
     ]
    }
   ],
   "source": [
    "# 2. Launch JumpStart model training\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "# Setup\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "model_id = \"xgboost-classification-model\"\n",
    "input_s3_uri = f\"s3://{bucket}/{clean_key}\"\n",
    "\n",
    "# JumpStart requires this input format\n",
    "input_data = {\n",
    "    \"training\": input_s3_uri\n",
    "}\n",
    "\n",
    "# Estimator\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"🚀 Launching JumpStart model training...\")\n",
    "estimator.fit(inputs=input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e55fb-caa7-45f6-8ee5-2320d1bdef92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
