{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Scikit-Learn In This Notebook\n",
    "## Saving Features into the SageMaker Feature Store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.8.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.8.1\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: protobuf==3.20.* in /opt/conda/lib/python3.11/site-packages (3.20.3)\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "!pip install --disable-pip-version-check -q tensorflow==2.8.1\n",
    "!pip install --disable-pip-version-check -q transformers==4.46.0\n",
    "!pip install protobuf==3.20.*\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1252/3809400061.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_cardio = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded cardio_train_cleaned from Athena\n",
      "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
      "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
      "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
      "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
      "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
      "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
      "\n",
      "   alco  active  cardio  \n",
      "0     0       1       0  \n",
      "1     0       1       1  \n",
      "2     0       0       1  \n",
      "3     0       1       1  \n",
      "4     0       0       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from pyathena import connect\n",
    "\n",
    "# Setup AWS session\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"  # Your actual S3 bucket\n",
    "role = get_execution_role()\n",
    "\n",
    "# Connect to Athena\n",
    "conn = connect(s3_staging_dir=f\"s3://{bucket}/athena/staging/\", region_name=region)\n",
    "\n",
    "# Fetch `cardio_train_cleaned` dataset\n",
    "query = \"SELECT * FROM cardiovale_db.cardio_train_cleaned\"\n",
    "df_cardio = pd.read_sql(query, conn)\n",
    "\n",
    "print(\"Successfully loaded cardio_train_cleaned from Athena\")\n",
    "print(df_cardio.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1252/126416555.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_quitline = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded quitline_fixed from Athena\n",
      "   year  date date_ref locationabbr locationdesc topictype  \\\n",
      "0  2020    12  Jul-Dec           GA      Georgia  Quitline   \n",
      "1  2020    12  Jul-Dec           MO     Missouri  Quitline   \n",
      "2  2020    12  Jul-Dec           MT      Montana  Quitline   \n",
      "3  2020    12  Jul-Dec           NJ   New Jersey  Quitline   \n",
      "4  2020    12  Jul-Dec           OK     Oklahoma  Quitline   \n",
      "\n",
      "            topicdesc  measuredesc         sub_measure              variable  \\\n",
      "0  Services Available  Medications    Nicotine Lozenge                         \n",
      "1  Services Available  Medications    Nicotine Lozenge                         \n",
      "2  Services Available  Medications  Bupropion (ZybanÂ®)                         \n",
      "3  Services Available  Medications    Nicotine Lozenge                         \n",
      "4  Services Available  Medications        Nicotine Gum  All Eligible Callers   \n",
      "\n",
      "   ... number_of_weeks_offered    limit_per_year comments  \\\n",
      "0  ...                     NaN                              \n",
      "1  ...                     NaN                              \n",
      "2  ...                     NaN                              \n",
      "3  ...                     NaN                              \n",
      "4  ...                     2.0  2 times per year            \n",
      "\n",
      "            geolocation             topictypeid topicid measureid  source  \\\n",
      "0   \"(32.83968109300048    -83.62758034599966)\"     QUI    900QUI  912MED   \n",
      "1  \"(38.635790776000476    -92.56630005299968)\"     QUI    900QUI  912MED   \n",
      "2   \"(47.06652897200047   -109.42442064499971)\"     QUI    900QUI  912MED   \n",
      "3   \"(40.13057004800049    -74.27369128799967)\"     QUI    900QUI  912MED   \n",
      "4   \"(35.47203135600046    -97.52107021399968)\"     QUI    900QUI  912MED   \n",
      "\n",
      "                        submeasureid displayorder  \n",
      "0  \"National Quitline Data Warehouse         None  \n",
      "1  \"National Quitline Data Warehouse         None  \n",
      "2  \"National Quitline Data Warehouse         None  \n",
      "3  \"National Quitline Data Warehouse         None  \n",
      "4  \"National Quitline Data Warehouse         None  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "#fetch quitline_fixed_.csv\n",
    "query = \"SELECT * FROM cardiovale_db.quitline_fixed LIMIT 5\"\n",
    "df_quitline = pd.read_sql(query, conn)\n",
    "\n",
    "print(\"Successfully loaded quitline_fixed from Athena\")\n",
    "print(df_quitline.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1252/1030601409.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged[\"number_of_weeks_offered\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# The cardio dataset does not have a 'State' column, but the quitline dataset is state-based.\n",
    "# To merge both datasets, we randomly assign a state to each row in the cardio dataset.\n",
    "# This ensures we can associate each patient with state-level quitline data.\n",
    "# However, this is an artificial assignment and does not reflect real patient locations.\n",
    "import random\n",
    "\n",
    "# Use correct column name from quitline dataset\n",
    "state_column = \"locationabbr\"\n",
    "\n",
    "# Extract unique states from quitline dataset\n",
    "states = df_quitline[state_column].dropna().unique().tolist()\n",
    "\n",
    "# Randomly assign a state to each row in the cardio dataset\n",
    "df_cardio[\"State\"] = [random.choice(states) for _ in range(len(df_cardio))]\n",
    "\n",
    "# Aggregating Quitline Data\n",
    "# ----------------------------\n",
    "# We summarize state-level quitline data by:\n",
    "# - Calculating the percentage of states where free services are offered\n",
    "# - Calculating the percentage of states with medical restrictions\n",
    "# - Taking the mean of the number of weeks offered for treatment\n",
    "\n",
    "df_quitline_summary = df_quitline.groupby(state_column).agg({\n",
    "    \"offered_for_free\": lambda x: (x == \"Yes\").mean(),  # Convert Yes/No to numerical (percentage)\n",
    "    \"medical_restrictions\": lambda x: (x == \"Yes\").mean(),\n",
    "    \"number_of_weeks_offered\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# Rename column for consistency\n",
    "df_quitline_summary.rename(columns={state_column: \"State\"}, inplace=True)\n",
    "\n",
    "# Merging Data\n",
    "# ----------------\n",
    "# We now merge the cardio dataset with the quitline summary based on 'State'.\n",
    "# This allows each row in df_cardio to inherit state-level information from quitline.\n",
    "\n",
    "df_merged = df_cardio.merge(df_quitline_summary, on=\"State\", how=\"left\")\n",
    "df_merged[\"number_of_weeks_offered\"].fillna(0, inplace=True)\n",
    "df_merged[\"State\"] = df_merged[\"State\"].astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>State</th>\n",
       "      <th>offered_for_free</th>\n",
       "      <th>medical_restrictions</th>\n",
       "      <th>number_of_weeks_offered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0   0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1   1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2   2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3   3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4   4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio State  offered_for_free  medical_restrictions  \\\n",
       "0     0       1       0    NJ               1.0                   0.0   \n",
       "1     0       1       1    NJ               1.0                   0.0   \n",
       "2     0       0       1    GA               0.0                   0.0   \n",
       "3     0       1       1    MO               0.0                   0.0   \n",
       "4     0       0       0    OK               1.0                   1.0   \n",
       "\n",
       "   number_of_weeks_offered  \n",
       "0                      0.0  \n",
       "1                      0.0  \n",
       "2                      0.0  \n",
       "3                      0.0  \n",
       "4                      2.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store requires an Event Time feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21T05:52:50Z\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "# timestamp = datetime.now().replace(microsecond=0).isoformat()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Features to SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Feature Store Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name=\"sagemaker-featurestore-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FeatureGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-21-05-56-35\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "feature_group_name = \"reviews-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "print(feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "# Define feature definitions for the merged cardio + quitline dataset\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name=\"id\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"age\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"gender\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"height\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"weight\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "    FeatureDefinition(feature_name=\"ap_hi\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"ap_lo\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"cholesterol\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"gluc\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"smoke\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"alco\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"active\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"cardio\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    \n",
    "    # Quitline features\n",
    "    FeatureDefinition(feature_name=\"State\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"offered_for_free\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "    FeatureDefinition(feature_name=\"medical_restrictions\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "    FeatureDefinition(feature_name=\"number_of_weeks_offered\", feature_type=FeatureTypeEnum.FRACTIONAL),\n",
    "\n",
    "    # Required timestamp for SageMaker Feature Store\n",
    "    FeatureDefinition(feature_name=\"event_time\", feature_type=FeatureTypeEnum.STRING)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-21-04-51-42', sagemaker_session=<sagemaker.session.Session object at 0x7f63bff91250>, feature_definitions=[FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None), FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None), FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None), FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None), FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None), FeatureDefinition(feature_name='split_type', feature_type=<FeatureTypeEnum.STRING: 'String'>, collection_type=None)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sess)\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify `record identifier` and `event time` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = \"review_id\"\n",
    "event_time_feature_name = \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set S3 Prefix for Offline Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-2025-03-21T04:51:42Z\n"
     ]
    }
   ],
   "source": [
    "prefix = \"reviews-feature-store-\" + timestamp\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Group\n",
    "\n",
    "The last step for creating the feature group is to use the `create` function. The online store is not created by default, so we must set this as `True` if we want to enable it. The `s3_uri` is the location of our offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:786782285170:feature-group/reviews-feature-group-21-04-51-42',\n",
       " 'ResponseMetadata': {'RequestId': '96e8b07c-cad4-4bbf-98ca-6641bb45053c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '96e8b07c-cad4-4bbf-98ca-6641bb45053c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '110',\n",
       "   'date': 'Fri, 21 Mar 2025 04:51:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:786782285170:feature-group/reviews-feature-group-21-04-51-42',\n",
       " 'FeatureGroupName': 'reviews-feature-group-21-04-51-42',\n",
       " 'RecordIdentifierFeatureName': 'review_id',\n",
       " 'EventTimeFeatureName': 'date',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'input_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'input_mask', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'segment_ids', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label_id', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'review_id', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'date', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'label', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'split_type', 'FeatureType': 'String'}],\n",
       " 'CreationTime': datetime.datetime(2025, 3, 21, 4, 51, 42, 783000, tzinfo=tzlocal()),\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-786782285170/reviews-feature-store-2025-03-21T04:51:42Z',\n",
       "   'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-786782285170/reviews-feature-store-2025-03-21T04:51:42Z/786782285170/sagemaker/us-east-1/offline-store/reviews-feature-group-21-04-51-42-1742532702/data'},\n",
       "  'DisableGlueTableCreation': False},\n",
       " 'ThroughputConfig': {'ThroughputMode': 'OnDemand'},\n",
       " 'RoleArn': 'arn:aws:iam::786782285170:role/LabRole',\n",
       " 'FeatureGroupStatus': 'Creating',\n",
       " 'ResponseMetadata': {'RequestId': '9f5139e9-5cbc-42f6-a65c-583e2254aca4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9f5139e9-5cbc-42f6-a65c-583e2254aca4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1592',\n",
       "   'date': 'Fri, 21 Mar 2025 04:51:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Feature Groups\n",
    "\n",
    "We use the boto3 SageMaker client to list all FeatureGroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.list_feature_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait For The Feature Group Creation Complete\n",
    "\n",
    "Creating a feature group takes time as the data is loaded. We will need to wait until it is created before you can use it. You can check status using the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup reviews-feature-group-21-04-51-42 successfully created.\n"
     ]
    }
   ],
   "source": [
    "wait_for_feature_group_creation_complete(feature_group=feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review The Records To Ingest Into Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input 0 of 3\n",
      "\n",
      "**37 tokens**\n",
      "['[CLS]', 'i', 'needed', 'an', '\"', 'anti', '##virus', '\"', 'application', 'and', 'know', 'the', 'quality', 'of', 'norton', 'products', '.', 'this', 'was', 'a', 'no', 'brain', '##er', 'for', 'me', 'and', 'i', 'am', 'glad', 'it', 'was', 'so', 'simple', 'to', 'get', '.', '[SEP]']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1045, 2734, 2019, 1000, 3424, 23350, 1000, 4646, 1998, 2113, 1996, 3737, 1997, 10770, 3688, 1012, 2023, 2001, 1037, 2053, 4167, 2121, 2005, 2033, 1998, 1045, 2572, 5580, 2009, 2001, 2061, 3722, 2000, 2131, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "4\n",
      "\n",
      "**review_id**\n",
      "ABCD12345\n",
      "\n",
      "**date**\n",
      "2025-03-21T04:51:42Z\n",
      "\n",
      "**label**\n",
      "5\n",
      "\n",
      "**42 tokens**\n",
      "['[CLS]', 'the', 'problem', 'with', 'elephant', '##drive', 'is', 'that', 'it', 'requires', 'the', 'use', 'of', 'java', '.', 'since', 'java', 'is', 'notorious', 'for', 'security', 'problems', 'i', 'have', '##it', 'removed', 'from', 'all', 'of', 'my', 'computers', '.', 'what', 'files', 'i', 'do', 'have', 'stored', 'are', 'photos', '.', '[SEP]']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1996, 3291, 2007, 10777, 23663, 2003, 2008, 2009, 5942, 1996, 2224, 1997, 9262, 1012, 2144, 9262, 2003, 12536, 2005, 3036, 3471, 1045, 2031, 4183, 3718, 2013, 2035, 1997, 2026, 7588, 1012, 2054, 6764, 1045, 2079, 2031, 8250, 2024, 7760, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "2\n",
      "\n",
      "**review_id**\n",
      "EFGH12345\n",
      "\n",
      "**date**\n",
      "2025-03-21T04:51:42Z\n",
      "\n",
      "**label**\n",
      "3\n",
      "\n",
      "**30 tokens**\n",
      "['[CLS]', 'terrible', ',', 'none', 'of', 'my', 'codes', 'worked', ',', 'and', 'i', 'can', \"'\", 't', 'un', '##ins', '##tal', '##l', 'it', '.', 'i', 'think', 'this', 'product', 'is', 'mal', '##ware', 'and', 'viruses', '[SEP]']\n",
      "\n",
      "**input_ids**\n",
      "[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499, 1010, 1998, 1045, 2064, 1005, 1056, 4895, 7076, 9080, 2140, 2009, 1012, 1045, 2228, 2023, 4031, 2003, 15451, 8059, 1998, 18191, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "0\n",
      "\n",
      "**review_id**\n",
      "IJKL2345\n",
      "\n",
      "**date**\n",
      "2025-03-21T04:51:42Z\n",
      "\n",
      "**label**\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 64\n",
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Records into Feature Store\n",
    "\n",
    "After the FeatureGroups have been created, we can put data into the FeatureGroups by using the `PutRecord` API. \n",
    "\n",
    "This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. \n",
    "\n",
    "The files will be written to the offline store within a few minutes of ingestion. To accelerate the ingestion process, we can specify multiple workers to do the job simultaneously. \n",
    "\n",
    "Use `put_record(...)` to put a single record in the FeatureGroup.\n",
    "\n",
    "Use `ingest(...)` to ingest the content of a pandas DataFrame to Feature Store. You can set the `max_worker` to the number of threads to be created to work on different partitions of the `data_frame` in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>split_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label split_type  \n",
       "0  2025-03-21T04:51:42Z      5      train  \n",
       "1  2025-03-21T04:51:42Z      3      train  \n",
       "2  2025-03-21T04:51:42Z      1      train  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_records = pd.DataFrame.from_dict(records)\n",
    "df_records[\"split_type\"] = \"train\"\n",
    "df_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast DataFrame `Object` to Supported Feature Store Data Type `String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_object_to_string(df_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>split_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1045, 2734, 2019, 1000, 3424, 23350, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>ABCD12345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 1996, 3291, 2007, 10777, 23663, 2003, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>EFGH12345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>IJKL2345</td>\n",
       "      <td>2025-03-21T04:51:42Z</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2734, 2019, 1000, 3424, 23350, 100...   \n",
       "1  [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...   \n",
       "2  [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id  review_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4  ABCD12345   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2  EFGH12345   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0   IJKL2345   \n",
       "\n",
       "                   date  label split_type  \n",
       "0  2025-03-21T04:51:42Z      5      train  \n",
       "1  2025-03-21T04:51:42Z      3      train  \n",
       "2  2025-03-21T04:51:42Z      1      train  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='reviews-feature-group-21-04-51-42', feature_definitions={'input_ids': {'FeatureName': 'input_ids', 'FeatureType': 'String'}, 'input_mask': {'FeatureName': 'input_mask', 'FeatureType': 'String'}, 'segment_ids': {'FeatureName': 'segment_ids', 'FeatureType': 'String'}, 'label_id': {'FeatureName': 'label_id', 'FeatureType': 'Integral'}, 'review_id': {'FeatureName': 'review_id', 'FeatureType': 'String'}, 'date': {'FeatureName': 'date', 'FeatureType': 'String'}, 'label': {'FeatureName': 'label', 'FeatureType': 'Integral'}, 'split_type': {'FeatureName': 'split_type', 'FeatureType': 'String'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f637f4a5bd0>, sagemaker_session=<sagemaker.session.Session object at 0x7f63bff91250>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f633ee15d50>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.ingest(data_frame=df_records, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait For Data In Offline Feature Store To Become Available\n",
    "\n",
    "Creating a feature group takes time as the data is loaded. We will need to wait until it is created before we can use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for data in offline store...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offline_store_contents = None\n",
    "\n",
    "while offline_store_contents is None:\n",
    "    objects_in_bucket = s3.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "    if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\n",
    "        offline_store_contents = objects_in_bucket[\"Contents\"]\n",
    "    else:\n",
    "        print(\"Waiting for data in offline store...\\n\")\n",
    "        sleep(60)\n",
    "\n",
    "print(\"Data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Wait For The Cell Above To Complete and show `Data available`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Record From Online Feature Store\n",
    "\n",
    "Use for OnlineStore serving from a FeatureStore. Only the latest records stored in the OnlineStore can be retrieved. If no Record with `RecordIdentifierValue` is found, then an empty result is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record_identifier_value = \"IJKL2345\"\n",
    "\n",
    "# featurestore_runtime.get_record(\n",
    "#     FeatureGroupName=feature_group_name, RecordIdentifierValueAsString=record_identifier_value\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (we can optionally turn it on/off while creating the FeatureGroup). We can create a training dataset by querying the data in the feature store. This is done by utilizing the auto-built Catalog and run an Athena query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create An Athena Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query = feature_group.athena_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The Feature Group Table Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_table = feature_store_query.table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Athena SQL Query\n",
    "\n",
    "Show Hive DDL commands to define or change structure of tables or databases in Hive. The schema of the table is generated based on the feature definitions. Columns are named after feature name and data-type are inferred based on feature type. \n",
    "\n",
    "Integral feature type is mapped to INT data-type. Fractional feature type is mapped to FLOAT data-type. String feature type is mapped to STRING data-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feature_group.as_hive_ddl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT input_ids, input_mask, segment_ids, label_id, split_type  FROM \"{}\" WHERE split_type='train' LIMIT 5\n",
    "\"\"\".format(\n",
    "    feature_store_table\n",
    ")\n",
    "\n",
    "print(\"Running \" + query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Athena Query\n",
    "The query results are stored in a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query.run(query_string=query_string, output_location=\"s3://\" + bucket + \"/\" + prefix + \"/query_results/\")\n",
    "\n",
    "feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Query Results\n",
    "\n",
    "Load query results in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "\n",
    "dataset = feature_store_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Feature Store\n",
    "\n",
    "![Feature Store](img/feature_store_sm_extension.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
